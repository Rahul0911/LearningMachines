{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0. Import libraries and install all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clone yolov5 github repo if needed\n",
    "#clone repo\n",
    "#!git clone https://github.com/ultralytics/yolov5\n",
    "\n",
    "#Install requirements\n",
    "#%cd yolov5\n",
    "#%pip install -r \"requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#Fixes an error when loading the yolov5 model\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Define all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load yolov5 trained model\n",
    "def load_custom_yolo(path_to_yolo_repo = r\"C:\\Users\\kurom\\Desktop\\AI_GBC\\S2\\DL_II\\Final_Project\\yolov5\"):\n",
    "    \"\"\"\n",
    "    Function to load a custom fine tunned yolov5 model. To run this function you need to have previously \n",
    "    cloned the yolov5 repo and provide the path to that folder, also the best.pt file must be \n",
    "    inside the yolov5 folder, if needed, manually copy and paste it in the desired location. \n",
    "    \"\"\"\n",
    "    #Change working directory to yolov5 repo, this is needed to load the model\n",
    "    os.chdir(path_to_yolo_repo)\n",
    "\n",
    "    #Load Model\n",
    "    custom_yolo = torch.hub.load('', 'custom', path='best.pt', source='local')\n",
    "\n",
    "    # Changing settings to prevent finding the faces multiple times\n",
    "    custom_yolo.conf = 0.5\n",
    "    custom_yolo.iou = 0.3\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"Yolo custom model successfully loaded...\")\n",
    "\n",
    "    return custom_yolo\n",
    "\n",
    "#Define a function to load pre trained CNN to perform image classification\n",
    "def load_img_clf(path_to_model_type = \"baseline\"):\n",
    "    \"\"\"\n",
    "    Function to load a pre trained model \n",
    "    in keras using .keras or .h5 file if needed modify the path locations specified for each model\n",
    "    Also you can add more models. The path_to_model_type arg controls the model to be loaded\n",
    "    \"\"\"\n",
    "    #Define path to model controlled by argument\n",
    "    if path_to_model_type == 'baseline':\n",
    "        path_to_model = r\"C:\\Users\\kurom\\Desktop\\AI_GBC\\S2\\DL_II\\Final_Project\\model.h5\"\n",
    "\n",
    "    elif path_to_model_type == 'custom_resnet_hub':\n",
    "        path_to_model = r\"C:\\Users\\kurom\\Desktop\\AI_GBC\\S2\\DL_II\\Final_Project\\custom_resnet_hub.pb\"\n",
    "\n",
    "    elif path_to_model_type == 'custom_VGG16_dcl':\n",
    "        path_to_model = r\"C:\\Users\\kurom\\Desktop\\AI_GBC\\S2\\DL_II\\Final_Project\\VGG_16_Cata.h5\"\n",
    "        \n",
    "    # Load the pre-trained model\n",
    "    clf_model = load_model(path_to_model)\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"Image Classifier successfully loaded ... \")\n",
    "\n",
    "    return clf_model\n",
    "\n",
    "#Define a function that executes preprocessing \n",
    "def image_preprocess(face, type='default', width=48, height=48):\n",
    "    \"\"\"\n",
    "    Defines a function to perform image pre processing \n",
    "    to a sliced_frame later passed to the classifier model\n",
    "    for inference, this sliced_frame contain only 1 face\n",
    "    type argument controls different types of pre processing\n",
    "    width and height control resizing\n",
    "    \"\"\"\n",
    "    if type == 'default':\n",
    "        # Resize the image\n",
    "        resized_face = cv2.resize(face, (width, height))\n",
    "\n",
    "        # Normalize the image (convert pixel values to [0, 1])\n",
    "        normalized_face = resized_face.astype(float) / 255.0\n",
    "\n",
    "        #Expand dims (1, w, h, channels)\n",
    "        normalized_face = np.expand_dims(normalized_face, axis=0)\n",
    "\n",
    "        return normalized_face\n",
    "    \n",
    "#Define a function to handle inference (predictions)\n",
    "def inference(model, face, label_dict_type='baseline'):\n",
    "    \"\"\"\n",
    "    Define a function that performs inference and \n",
    "    returns the emotion string i.e \"Happy\" and \n",
    "    the probability associated i.e \"0.87\" for a single sliced_frame i.e a face\n",
    "    \"\"\"\n",
    "    #Label map depends on the model we are using for inference\n",
    "    if label_dict_type == 'resnet':\n",
    "        label_dict = {0: 'Angry', 1: 'Fearful', 2: 'Happy', 3: 'Neutral', 4: 'sad', 5: 'surprised'}\n",
    "\n",
    "    elif label_dict_type == 'baseline':\n",
    "        label_dict={0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'sad', 5: 'Surprised', 6: 'Neutral'}\n",
    "\n",
    "    elif label_dict_type == 'vgg16':\n",
    "        label_dict={0: 'Angry', 1: 'Fearful', 2: 'Happy', 3: 'Neutral', 4: 'sad', 5: 'surprised'}\n",
    "    \n",
    "    #Make prediction\n",
    "    pred = model.predict(face)\n",
    "    label_key = pred.argmax(axis=1)[0]\n",
    "    label_str = label_dict[label_key]\n",
    "    score = round(pred[0][label_key], 2)\n",
    "\n",
    "    return label_str, str(score)\n",
    "\n",
    "#Define main loop that will control the flow of this script\n",
    "def main_loop(face_detector, emotion_clf, emotion_label_dict_type='baseline', process_frames=1, font = cv2.FONT_HERSHEY_DUPLEX, width=48, height=48, model_caption='baseline'):\n",
    "    \"\"\"\n",
    "    Define function for controlling main loop\n",
    "    process_frames controls the number of frames to be \n",
    "    processed before skipping next frame to reduce\n",
    "    processing load\n",
    "\n",
    "    face_detector -> yolo fine tuned model\n",
    "    emotion_clf -> keras pre trained model for img (emotion) classification \n",
    "    emotion_label_dict_type -> used in inference() call controls the mapping of labels to emotions\n",
    "    process_frames -> controlls number of frames to be processed before skipping a frame this is to reduce\n",
    "    processing load on pc, if you don't understand well just leave it on 1.\n",
    "    font -> controlls font to write on live video feed, this one is a good font, leave it like that\n",
    "    width, height -> used in image_preprocess(), controlls shape that a face should be resized before feeding it to classifier model\n",
    "    remember this depends on the model you are using, make sure to use the same shape as the classifier you choose\n",
    "    model_caption -> controls the descriptive caption written in green box at the top of live video feed, \n",
    "    manually adjust it regarding to the model you are testing example \"custom VGG 16 Cata\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the video capture from camera\n",
    "    cap = cv2.VideoCapture(0)  # Use 0 for the default camera\n",
    "\n",
    "    #Control frame skipping to reduce processing load on pc\n",
    "    process_frame = True\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the camera\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Check if the frame should be processed or skipped\n",
    "        if process_frame:\n",
    "            # 1st detect faces in frame\n",
    "            faces = face_detector(frame)\n",
    "            \n",
    "            #2nd if at least 1 face was detected\n",
    "            if len(faces.pred[0]) > 0:\n",
    "                #For each detected face\n",
    "                i = 1 #detected face count\n",
    "                for face in faces.pred[0]:\n",
    "                    #Retrieve bbox coordinates for this face\n",
    "                    x_1 = int(face[0])\n",
    "                    y_1 = int(face[1])\n",
    "                    x_2 = int(face[2])\n",
    "                    y_2 = int(face[3])\n",
    "\n",
    "                    #Retrieve width of bbox\n",
    "                    w = abs(x_1 - x_2)\n",
    "\n",
    "                    #Crop face\n",
    "                    sliced_face = frame[y_1:y_2, x_1:x_2]\n",
    "\n",
    "                    #pre process sliced_face for img clf\n",
    "                    processed_face = image_preprocess(face=sliced_face, type='default', width=width, height=height)\n",
    "\n",
    "                    #make emotion prediction and retrieve emotion string and score for that emotion\n",
    "                    emotion_str, e_score_str = inference(model=emotion_clf, face=processed_face, label_dict_type=emotion_label_dict_type)\n",
    "\n",
    "                    #Retrieve probability score for object detection (this is different from img classification)\n",
    "                    f_score_str = str(round(float(face[4]), 2))\n",
    "\n",
    "                    #Draw bbox rectangle for face\n",
    "                    cv2.rectangle(frame, (x_1, y_1), (x_2, y_2), color=(0,0,255), thickness=1)\n",
    "                    #Draw face detection label rectangle\n",
    "                    cv2.rectangle(frame, (x_1, y_1-23), (x_1+w, y_1-7), color=(0,0,255), thickness=-1)\n",
    "                    #Draw emotion detection label rectangle\n",
    "                    cv2.rectangle(frame, (x_1, y_1-45), (x_1+w, y_1-28), color=(255,0,0), thickness=-1)\n",
    "                    #Draw model label rectangle\n",
    "                    cv2.rectangle(frame, (220, 1), (420, 25), color=(0,255,0), thickness=-1)\n",
    "\n",
    "                    #Write face detection caption\n",
    "                    cv2.putText(img=frame, text='Face #'+str(i)+' '+f_score_str, org=(x_1,y_1-10), \n",
    "                                fontFace=font, fontScale=0.5, color=(255,255,255), thickness=1, \n",
    "                                lineType=cv2.LINE_AA, bottomLeftOrigin=False)\n",
    "                    \n",
    "                    #Write emotion detection caption\n",
    "                    cv2.putText(img=frame, text=emotion_str+''+e_score_str, org=(x_1,y_1-33), \n",
    "                                fontFace=font, fontScale=0.5, color=(255,255,255), thickness=1, \n",
    "                                lineType=cv2.LINE_AA, bottomLeftOrigin=False)\n",
    "                    \n",
    "                    #Write model caption\n",
    "                    cv2.putText(img=frame, text=model_caption, org=(260,20), \n",
    "                                fontFace=font, fontScale=0.5, color=(0,0,0), thickness=1, \n",
    "                                lineType=cv2.LINE_AA, bottomLeftOrigin=False)\n",
    "                    \n",
    "                    i = i+1 #Update detected faces counter\n",
    "                    #Don't detect more than specified number of faces\n",
    "                    if i >= 3:\n",
    "                        break\n",
    "                frame_count += 1 #update frame skipping control counter\n",
    "                #when number of processed frames = the number of frames to be processed\n",
    "                if frame_count == process_frames:\n",
    "                    frame_count = 0 #reset counter\n",
    "                    process_frame = False #skip next frame\n",
    "            else: #When no faces are detected\n",
    "                #Draw no face detected label rectangle\n",
    "                cv2.rectangle(frame, (20, 1), (255, 25), color=(0,0,255), thickness=-1)\n",
    "                #Draw model label rectangle\n",
    "                cv2.rectangle(frame, (220, 1), (420, 25), color=(0,255,0), thickness=-1)\n",
    "\n",
    "                #Draw no face detected caption\n",
    "                cv2.putText(img=frame, text='No faces detected', org=(20,20), fontFace=font, \n",
    "                            fontScale=0.8, color=(255,255,255), thickness=1, lineType=cv2.LINE_AA, \n",
    "                            bottomLeftOrigin=False)\n",
    "                #Write model caption\n",
    "                cv2.putText(img=frame, text=model_caption, org=(260,20), \n",
    "                            fontFace=font, fontScale=0.5, color=(0,0,0), thickness=1, \n",
    "                            lineType=cv2.LINE_AA, bottomLeftOrigin=False)\n",
    "\n",
    "                frame_count += 1 #update frame skipping control counter\n",
    "                if frame_count == process_frames:\n",
    "                    frame_count = 0 #reset counter\n",
    "                    process_frame = False #skip next frame\n",
    "        else:\n",
    "            # Skip the frame        \n",
    "            # Set process_frame to True to process the next frame\n",
    "            process_frame = True\n",
    "            continue\n",
    "\n",
    "        # Display the frame with bounding boxes, rectangles and captions\n",
    "        cv2.imshow('Live Video', frame)\n",
    "\n",
    "        # Check for q key press to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the video capture and close the OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kurom\\anaconda3\\envs\\dl-venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "YOLOv5  v7.0-294-gdb125a20 Python-3.10.8 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Yolo custom model successfully loaded...\n"
     ]
    }
   ],
   "source": [
    "#First always read yolo, this step is static unless we try a different object detector for face detection, not for now\n",
    "custom_yolo_model = load_custom_yolo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create different calls for evaluating different model, first always load model and then call main loop function, see some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline\n",
    "clf_model = load_img_clf(path_to_model_type='baseline')\n",
    "main_loop(face_detector=custom_yolo_model, emotion_clf=clf_model, emotion_label_dict_type='baseline', \n",
    "          process_frames=1, font = cv2.FONT_HERSHEY_DUPLEX, width=48, height=48, model_caption='baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16\n",
    "clf_model = load_img_clf(path_to_model_type='custom_VGG16_dcl')\n",
    "main_loop(face_detector=custom_yolo_model, emotion_clf=clf_model, emotion_label_dict_type='vgg16', \n",
    "          process_frames=1, font=cv2.FONT_HERSHEY_DUPLEX, width=224, height=224, model_caption='custom_VGG16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet\n",
    "clf_model = load_img_clf(path_to_model_type='custom_resnet_hub')\n",
    "main_loop(face_detector=custom_yolo_model, emotion_clf=clf_model, emotion_label_dict_type='resnet', \n",
    "          process_frames=1, font=cv2.FONT_HERSHEY_DUPLEX, width=48, height=48, model_caption='custom_resnet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
